{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca96fadf",
   "metadata": {},
   "source": [
    "Copyright 2023 The Johns Hopkins University Applied Physics Laboratory LLC\n",
    "\n",
    "All rights reserved.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this \n",
    "software and associated documentation files (the \"Software\"), to deal in the Software \n",
    "without restriction, including without limitation the rights to use, copy, modify, \n",
    "merge, publish, distribute, sublicense, and/or sell copies of the Software, and to \n",
    "permit persons to whom the Software is furnished to do so.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, \n",
    "INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR \n",
    "PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE \n",
    "LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, \n",
    "TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE \n",
    "OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01715644",
   "metadata": {},
   "source": [
    "# MICrONS Co-registration and Visualization\n",
    "\n",
    "This notebook uses the `dandi`-hosted [MICrONS functional data](https://dandiarchive.org/dandiset/000402/draft) and `bossdb`-hosted [MICrONS structural data](https://bossdb.org/microns/minnie) to examine and visualize co-registered cells.\n",
    "\n",
    "More specifically, we update the PlaneSegmentation tables from the MICrONS DandiSet with the automated coregistration IDs in the `apl_functional_coreg_forward_v5` CAVE table. This notebook demonstrates updating these IDs for a random session and scan, plotting two functional traces for connected neurons, and visualizing those two neurons in Neuroglancer. To analyze a specific session and scan, switch out the call to `random()` with the session and scan numbers of interest.\n",
    "\n",
    "This notebook requires the file 'ScanUnit.pkl' to properly execute. This file contains a dataframe mapping the mask IDs (which are session/scan/field specific) to Unit IDs (which are session/sca specific.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8ed0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dandi.dandiapi import DandiAPIClient\n",
    "from caveclient import CAVEclient\n",
    "\n",
    "from fsspec.implementations.cached import CachingFileSystem\n",
    "from fsspec import filesystem\n",
    "from h5py import File\n",
    "from pynwb import NWBHDF5IO\n",
    "from pynwb.file import NWBFile\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "from pynwb.ophys import PlaneSegmentation\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484bd8a-41bb-4f65-854e-794241a9674e",
   "metadata": {},
   "source": [
    "### Open the CAVE table with the structural / functional coregistration metadata\n",
    "\n",
    "The table below lists metadata for each neuron nucleus id which has been coregistered with a functional unit id. The `target_id` field is unique to a specific neuron. The `session`, `scan_idx`, and `unit_id` fields can be used to match a nucleus id to a functional mask ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ed887-912f-43a1-bd84-090d83c04d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cave = CAVEclient(\"minnie65_phase3_v1\")\n",
    "coreg = cave.materialize.query_table(\"apl_functional_coreg_forward_v5\")\n",
    "coreg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f2d86-e40b-4fd8-8aaf-4dbff94756c3",
   "metadata": {},
   "source": [
    "### Open the metadata table for the functional recording\n",
    "\n",
    "This table has been pulled from a [DataJoint](https://datajoint.com/) database for convenience. The extra step is necessary because the dandiset identifies neurons by their mask IDs, not their unit IDs. Unit IDs are unique across a Dandi NWB file while mask IDs are unique within a field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7655c-db56-4eef-b39b-9b7a233bb17b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scan_units = pd.read_pickle(\"./ScanUnit.pkl\")\n",
    "scan_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8c504-18ec-4efd-8b1f-e397df594077",
   "metadata": {},
   "source": [
    "### Open the dandiset of interest\n",
    "\n",
    "The Microns two-photon calcium scans are found in dandiset 000402. All sessions and scans can be browsed at https://dandiarchive.org/dandiset/000402/0.230307.2132/files?location=. We will choose an entry at random from the coregistration table and open the NWB file which holds its functional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a1966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coreg_row = coreg.sample(1).iloc[0] # randomly select a row from the coregisration table\n",
    "\n",
    "session_no, scan_no, unit_id = coreg_row[\"session\"], coreg_row[\"scan_idx\"], coreg_row[\"unit_id\"] \n",
    "\n",
    "# access scan unit row associated with the coregistration one\n",
    "scan_unit_row = scan_units[(scan_units['session']==session_no)&(scan_units['scan_idx']==scan_no)&(scan_units['unit_id']==unit_id)]\n",
    "\n",
    "dandiset_id = \"000402\"\n",
    "\n",
    "display((session_no, scan_no, unit_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba138bf1-2ca2-42de-b124-5ae342eed3e0",
   "metadata": {},
   "source": [
    "Since each file is tens of GB large, use the NWB streaming capability to open the file without reading all of it into memory at once, as explained at https://pynwb.readthedocs.io/en/stable/tutorials/advanced_io/streaming.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd47621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the location of the file on DANDI\n",
    "def get_NWBFile(session_no, scan_no):\n",
    "    file_path = f\"sub-17797/sub-17797_ses-{session_no}-scan-{scan_no}_behavior+image+ophys.nwb\"\n",
    "    with DandiAPIClient() as client:\n",
    "        asset = client.get_dandiset(dandiset_id, 'draft').get_asset_by_path(file_path)\n",
    "        s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "\n",
    "    # First, create a virtual filesystem based on the http protocol\n",
    "    fs = filesystem(\"http\")\n",
    "\n",
    "    # Create a cache to save downloaded data to disk (optional)\n",
    "    fs = CachingFileSystem(\n",
    "        fs=fs,\n",
    "        cache_storage=\"nwb-cache\",  # Local folder for the cache\n",
    "    )\n",
    "\n",
    "    # Next, open the file with NWBHDF5IO\n",
    "    file_system = fs.open(s3_url, \"rb\")\n",
    "    file = File(file_system, mode=\"r\")\n",
    "    io = NWBHDF5IO(file=file, load_namespaces=True)\n",
    "\n",
    "    microns_data = io.read()\n",
    "    return microns_data\n",
    "\n",
    "microns_data = get_NWBFile(session_no, scan_no)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f3b7662-27fc-41bf-b710-a57acc13abb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Update the NWB File\n",
    "\n",
    "The `NWBFile` object contains all the metadata associated with the dandiset. The attribute `processing -> ophys -> data_interfaces -> ImageSegmentation -> plane_segmentations` contains the structural ids associated with the image masks. The first step this notebook will take is to update each `PlaneSegmentation` object using the new structural ids which were published in 2023. These are found in the CAVE table `apl_functional_coreg_forward_v5`, column `target_id`. While we're in there, also add the unit IDs and other metadata from the CAVE table as columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12698eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following cell depends on these functions\n",
    "def create_new_plane_segmentation(old, df, descriptions):\n",
    "    ps = PlaneSegmentation(\n",
    "        name=old.name, \n",
    "        description=old.description, \n",
    "        imaging_plane=old.imaging_plane,\n",
    "        id=df.index.tolist()\n",
    "    )\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in old.colnames:\n",
    "            old_col = find_column_by_name(old, col)\n",
    "            ps.add_column(name=old_col.name, description=old_col.description, data=df[col].tolist())\n",
    "        else:\n",
    "            ps.add_column(name=col, description=descriptions[col], data=df[col].tolist())\n",
    "    return ps\n",
    "        \n",
    "\n",
    "def find_column_by_name(table,col_name):\n",
    "    for c in table.columns:\n",
    "        if c.name == col_name:\n",
    "            return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceaad08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update the PlaneSegmentation objects in the NWB file\n",
    "def update_microns_nwb_file(\n",
    "    nwb: NWBFile,\n",
    "    coreg,\n",
    "    scan_units,\n",
    "):\n",
    "        \n",
    "    session, scan_idx = int(nwb.session_id.split('-')[0]), int(nwb.session_id.split('-')[2])\n",
    "    if session == 5 and scan_idx == 7:\n",
    "        print(\"Error: This file does not contain a unit_id column\")\n",
    "        return \n",
    "    scan_units_modified = scan_units[(scan_units['session']==session) & (scan_units['scan_idx']==scan_idx)]\n",
    "    \n",
    "    image_segmentation = nwb.processing[\"ophys\"].data_interfaces[\"ImageSegmentation\"]\n",
    "    \n",
    "    all_ps = list(image_segmentation.plane_segmentations)\n",
    "    for ps_name in tqdm(all_ps):\n",
    "        \n",
    "        ps = image_segmentation.plane_segmentations.pop(ps_name)\n",
    "        field = int(ps_name[-1])\n",
    "        field_scan_units = scan_units_modified[scan_units_modified['field'] == field]\n",
    "        ps_df = ps[:]\n",
    "        ps_df['mask_id'] = ps_df.index\n",
    "        ps_df_with_units = ps_df.merge(field_scan_units, on='mask_id', how='left').drop(columns=[\n",
    "            'mask_id', 'session', 'scan_idx'\n",
    "            # 'mask_id', 'session', 'scan_idx', 'field'\n",
    "        ])\n",
    "        \n",
    "        coreg_units = coreg[\n",
    "            (coreg['session']==session) & \n",
    "            (coreg['scan_idx']==scan_idx) & \n",
    "            (coreg['field'] == field)\n",
    "        ][['target_id', 'unit_id']]\n",
    "        \n",
    "        if len(coreg_units):\n",
    "            ps_df_with_units = ps_df_with_units.merge(coreg_units, on='unit_id').rename(\n",
    "                columns={\n",
    "                    'target_id': 'auto_match_cave_nuclei_id', \n",
    "                    'cave_ids': 'manual_match_cave_nuclei_id'\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        description = {x: \"Placeholder\" for x in ps_df_with_units.columns}\n",
    "        new_ps = create_new_plane_segmentation(ps, ps_df_with_units, description)\n",
    "        image_segmentation.plane_segmentations.add(new_ps)\n",
    "        \n",
    "    return nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc04e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If this fails with \"This file does not contain a unit_id column\", then session 5 scan 7 was chosen.\n",
    "# This file does not contain unit_ids, so a new random neuron must be chosen.\n",
    "# Rerun the cells in Open the Dandiset of Interest to choose a new random neuron, then try this cell again.\n",
    "microns_data = update_microns_nwb_file(microns_data, coreg, scan_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ded545-8fd3-4c8b-b932-3f30ad559a94",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "The functional data contains multiple types of data for a single mask ID. Plot the image of the mask and its fluorescence trace together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ae432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Return the mask image, fluorescence trace values, and timestamps for a given unit id\n",
    "def get_time_series_data(unit_id):\n",
    "    mask_id = 0\n",
    "    for plane_seg in range(1, 9):\n",
    "        image_seg = microns_data.processing[\"ophys\"].data_interfaces[\"ImageSegmentation\"].plane_segmentations[f\"PlaneSegmentation{plane_seg}\"]\n",
    "        try:\n",
    "            mask_id = image_seg.get(\"unit_id\").data.index(unit_id)\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if mask_id == 0:\n",
    "        return Exception(f\"Unit Id {unit_id} not found\")\n",
    "\n",
    "    # Get fluorescence of the table number\n",
    "    fluor = microns_data.processing[\"ophys\"].data_interfaces[\"Fluorescence\"].roi_response_series[f\"RoiResponseSeries{plane_seg}\"]\n",
    "\n",
    "    # Pull the data we are interested in\n",
    "    mask_image = image_seg.image_mask[mask_id]\n",
    "    fluor_data = fluor.data[:, mask_id]\n",
    "    timestamps = fluor.timestamps[:]\n",
    "\n",
    "    return mask_image, fluor_data, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8708986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the segmentation and fluorescence trace for the unit id\n",
    "mask_image, fluor, timestamps = get_time_series_data(unit_id)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n",
    "fig.suptitle(f'session: {session_no}, scan: {scan_no}, unit_id: {unit_id}')\n",
    "ax1.imshow(mask_image)\n",
    "ax2.plot(timestamps, fluor)\n",
    "ax2.set_xlabel(\"time (s)\")\n",
    "ax2.set_ylabel(\"response magnitude\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d74cd342",
   "metadata": {},
   "source": [
    "## Neuroglancer Visualization \n",
    "Generate JSON states associated with the functional and structural information for a unit. Unit info is formatted in the tuple `(session_id, scan_id, field_id, unit_field_id)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc76363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ng_utils import generate_functional_ng_state, generate_structural_ng_state, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1395f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The field ID is a new identifier not mentioned yet in this notebook. It is important for pulling the correct Neuroglancer layers.\n",
    "# It was added as a column when we modified the PlaneSegmentation objects. Index into this column to grab the correct field ID.\n",
    "def get_field_id(unit_id):\n",
    "    mask_id = 0\n",
    "    field_id = 0\n",
    "    for plane_seg in range(1, 9):\n",
    "        image_seg = microns_data.processing[\"ophys\"].data_interfaces[\"ImageSegmentation\"].plane_segmentations[f\"PlaneSegmentation{plane_seg}\"]\n",
    "        try:\n",
    "            mask_id = image_seg.get(\"unit_id\").data.index(unit_id)\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            field_id = image_seg.get(\"field\").data[mask_id]\n",
    "            break\n",
    "\n",
    "    if mask_id == 0:\n",
    "        return Exception(f\"Unit Id {unit_id} not found\")\n",
    "\n",
    "    return field_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85608e8-597a-4d6d-a170-78f58f960f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Neuroglancer JSON states\n",
    "# States will be saved to a file that can be copied into Neuroglancer's state editor\n",
    "\n",
    "unit_info = (session_no, scan_no, get_field_id(unit_id), unit_id)\n",
    "\n",
    "\"\"\"\n",
    "Functional Test\n",
    "\"\"\"\n",
    "funct_ng_state, target_id = generate_functional_ng_state(unit_info)\n",
    "save_json(f'functional_state_{\"_\".join([str(x) for x in unit_info])}.json', funct_ng_state)\n",
    "\n",
    "\"\"\"\n",
    "Structural Test\n",
    "\"\"\"\n",
    "struct_ng_state = generate_structural_ng_state(target_id)\n",
    "save_json(f'structural_state_id_{\"_\".join([str(x) for x in unit_info])}.json', struct_ng_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, run this cell to open in browser\n",
    "import webbrowser\n",
    "funct_url = 'https://neuroglancer.bossdb.io/#!' + urllib.parse.quote(json.dumps(funct_ng_state))\n",
    "struct_url = 'https://neuroglancer.bossdb.io/#!' + urllib.parse.quote(json.dumps(struct_ng_state))\n",
    "webbrowser.open(funct_url)\n",
    "webbrowser.open(struct_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be05b9c-14b9-4cbe-a397-28df9f28d86b",
   "metadata": {},
   "source": [
    "### Connectivity-informed functional analysis\n",
    "\n",
    "We can use the `pt_root_id` column in the coregistration table we pulled at the very beginning of the notebook to generate a list of neurons which synapse onto our neuron of interest, along with the quantity and pre/post designation of each synapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71147e46-5ea3-486b-9ee8-cf6c87159387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_synapse_map(pt_root_id):\n",
    "    return {\n",
    "        'pre': cave.materialize.query_table(\"synapses_pni_2\", \n",
    "            filter_equal_dict={\"post_pt_root_id\": str(pt_root_id)})['pre_pt_root_id'].value_counts().to_dict(),\n",
    "        'post': cave.materialize.query_table(\"synapses_pni_2\", \n",
    "            filter_equal_dict={\"pre_pt_root_id\": str(pt_root_id)})['post_pt_root_id'].value_counts().to_dict()\n",
    "    }\n",
    "\n",
    "pt_root_id = coreg.iloc[coreg_row][\"pt_root_id\"]\n",
    "synapse_map = generate_synapse_map(pt_root_id)\n",
    "\n",
    "# Remove neuron cycles first if they exist.\n",
    "del synapse_map['pre'][pt_root_id]\n",
    "del synapse_map['post'][pt_root_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf498d-5e9b-472d-b9cc-97a8c2dc907a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find a functionally characterized neuron in the list of post synaptic neurons\n",
    "for post_pt_root_id in synapse_map['post']:\n",
    "    coreg_row_2 = coreg.loc[coreg[\"pt_root_id\"] == post_pt_root_id].head(1)\n",
    "    if len(coreg_row_2) > 0: break\n",
    "    \n",
    "# Repeat the steps to pull the NWB file for the neuron\n",
    "scan_unit_row_2 = scan_units.loc[coreg_row_2.index[0]]\n",
    "\n",
    "session_no_2 = scan_unit_row_2[\"session\"]\n",
    "scan_no_2 = scan_unit_row_2[\"scan_idx\"]\n",
    "unit_id_2 = scan_unit_row_2[\"unit_id\"]\n",
    "\n",
    "microns_data_2 = get_NWBFile(session_no_2, scan_no_2)\n",
    "microns_data_2 = update_microns_nwb_file(microns_data_2, coreg, scan_units)\n",
    "#microns_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db8e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the two functional traces on top of each other\n",
    "\n",
    "mask_image2, fluor2, timestamps2 = get_time_series_data(unit_id_2)\n",
    "\n",
    "plt.plot(fluor, label=f\"Session {session_no}, Scan {scan_no}, Unit Id {unit_id}\", alpha=0.5)\n",
    "plt.plot(fluor2, label=f\"Session {session_no_2}, Scan {scan_no_2}, Unit Id {unit_id_2}\", alpha=0.5)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Response Magnitude\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1a3b2ec9f69649c0500eeac98d17fbbf901b93530ebe3fc8fe808263e60f6de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
